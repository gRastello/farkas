\documentclass[italian, letter paper, 12pt, reqno]{article}
\usepackage[italian]{babel}

\setlength{\evensidemargin}{0.1in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\textwidth}{6.3in}
\setlength{\topmargin}{0.0in}
\setlength{\textheight}{8.5in}
\setlength{\headheight}{0in}

% Links and references.
\usepackage{xcolor}
\definecolor{Myblue}{rgb}{0,0,0.6}
\usepackage[a4paper,colorlinks,citecolor=Myblue,linkcolor=Myblue,urlcolor=Myblue,pdfpagemode=None]{hyperref}

% Necessities for math.
\usepackage{amsmath, amscd, amssymb, mathrsfs, accents, amsfonts, amsthm}

\newtheoremstyle{myteo}{\topsep}{\topsep}
	{}
	{}
	{\bfseries}
	{.}
	{2pt}
	{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\theoremstyle{myteo}

\newtheorem{theorem}{Teorema}[section]
\newtheorem{proposition}[theorem]{Proposizione}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollario}
\newtheorem{definition}[theorem]{Definizione}
\newtheorem{example}[theorem]{Esempio}
\newtheorem{remark}[theorem]{Osservazione}

\numberwithin{equation}{section}

\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{fadings}

% Figures stuff.
\usepackage{caption}
% \renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}

% Lists stuff.
\usepackage{enumitem}
\setenumerate{label=(\arabic*)}

% Commands.
\newcommand{\dual}[1]{#1^{\text{op}}}

\DeclareMathOperator{\im}{im}

% Useless stuff.
\usepackage{epigraph}

\begin{document}
\title{Appunti sul Teorema di Dualità e sul Lemma di Farkas}
\author{Gabriele Rastello}
\maketitle
\tableofcontents

\section{Problemi lineari}
\label{sec:problemi_lineari}
\epigraph{Linear programming, surprisingly, is not directly related to computer programming.}{\textit{Jiri Matousek, Bernd Garter}}
Sono problemi lineari tutti quei problimi in cui ci si prefigge di trovare il valore massimo (o minimo) che una certa funzione lineare di \(n\) variabili può assumere, dato un qualche numero di vincoli (anche essi lineari) su queste variabili.
Prima di definire formalmente un problema lineare consideriamo un esempio.

\begin{example}
  \label{es:problema_lineare}
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & x_1 + x_2\\
      \text{rispetto ai vincoli} & x_1, x_2\geq 0\\
                        & x_2 - x_1 \leq 1\\
                        & x_1 + 6x_2 \leq 15\\
                        & 4x_1 - x_2 \leq 10
    \end{array}
  \end{equation*}
  In \(\mathbb{R}^2\) ogni vincolo individua un semipiano.
  La zona di \(\mathbb{R}^2\) su cui vogliamo massimizzare \(x_1+x_2\) è dunque l'intersezione di tutti questi semipiani ed è rappresentata in Figura \ref{fig:problema_lineare}.
  Osserviamo che quest'area non è vuota e che è un poligono convesso.
  Esiste dunque una coppia \((x_1^*,x_2^*)\) che massimizza \(x_1+x_2\); la coppia in questione può essere ottenuta cercando quale punto del poligono si trova ``più distante'' nella direzione di massima crescita della funzione (data dal suo gradiente \((1, 1)\)).
  Otteniamo così \(x_1^*=3, x_2^*=2\) e infine che il valore massimo di \(x_1+x_2\) rispetto ai vincoli dati è \(5\).

  \begin{figure}
    \begin{center}
      \begin{tikzpicture}
        \filldraw[fill=green!20!white]
        (0, 0) -- (0, 1) -- (9/7, 16/7) -- (3, 2) -- (10/4, 0);

        \draw[->] (-1, 0) -- (4, 0) node[right]{\(x_1\)};
        \draw[->] (0, -1) -- (0, 3.5) node[above]{\(x_2\)};

        \draw[domain = -.75:3.25] plot (\x, {\x + 1});
        \draw[domain = -.75:4] plot (\x, {(15 - \x)/6});
        \draw[domain = 2.25:3.5] plot (\x, {4*\x - 10});

        \filldraw (3, 2) circle (2pt) node[above right]{\((3,2)\)};
      \end{tikzpicture}
    \end{center}
    \caption{}
    \label{fig:problema_lineare}
  \end{figure}
\end{example}

\begin{definition}
  \label{def:problema_lineare}
  Un \textbf{problema lineare} consiste in una funzione lineare di \(n\) variabili detta \textbf{funzione obiettivo} (o \textbf{funzione di costo}) e in un insieme di \(m\) vincoli lineari.
  La funzione obiettivo ha la forma \(\mathbf{c}^T\mathbf{x} = c_1x_1+\ldots+c_nx_n\) per qualche \(\mathbf{c}\in\mathbb{R}^n\); lo stesso si applica ai vincoli.
  Dare un problema lineare è allora equivalente a dare un vettore \(\mathbf{c}\in\mathbb{R}^n\), una matrice \(A\in\mathbb{R}^{m\times n}\) e un vettore \(\mathbf{b}\in\mathbb{R}^m\).
  Scriveremo compattamente
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}.
    \end{array}
  \end{equation*}
\end{definition}

\begin{remark}
  \label{oss:definizione_generale}
  La Definizione \ref{def:problema_lineare} è del tutto generale.
  Infatti un problema di minimizzazione può essere trasformato in uno di massimizzazione cambiando segno alla funzione obiettivo.
  I vincoli espressi tramite un'uguaglianza \(\mathbf{a}^T\mathbf{x}=b\) sono equivalenti alla coppia di disuguaglianze \(\mathbf{a}^T\mathbf{x}\geq b\), \(\mathbf{a}^T\mathbf{x}\leq b\).
  Ed infine le disuguaglianze possono essere espresse tutte quante nella forma \(\mathbf{a}^T\mathbf{x}\leq b\).
\end{remark}

\begin{definition}
  \label{def:soluzioni}
  Un vettore \(\mathbf{x}\in\mathbb{R}^n\) che soddisfa tutti i vincoli di un problema lineare è una \textbf{soluzione possibile} per il problema.
  Un problema è \textbf{soddisfacibile} se ammette una soluzione possibile ed è \textbf{insoddisfacibile} altrimenti. Una soluzione possibile \(\mathbf{x}^*\in\mathbb{R}^n\) è una \textbf{soluzione ottimale} se \(\mathbf{c}^T\mathbf{x}^*\) è massimo tra i valori \(\mathbf{c}^T\mathbf{x}\) con \(\mathbf{x}\) soluzione possibile.
\end{definition}

\begin{remark}
  \label{oss:soluzioni_ottimali_illimitate}
  Va osservato che, generalmente, un sistema lineare può avere più di una soluzione ottimale; come esempio si considerino i vincoli dell'Esercizio \ref{es:problema_lineare} applicati però alla funzione obiettivo \(\frac{1}{6}x_1+x_2\).
  È inoltre vero che, anche se un sistema è soddisfacibile, possono non esistere soluzioni ottimali; come esempio basta rimuovere i vincoli \(x_1+6x_2\leq15\) e \(4x_1-x_2\leq10\) dall'Esercizio \ref{es:problema_lineare}.
\end{remark}

\begin{definition}
  \label{def:problemi_limitati_e_illimitati}
  Se un problema lineare ammette (almeno) una soluzione ottimale allora è detto \textbf{limitato}; se non ne ammette viene detto \textbf{illimitato}.
\end{definition}

Concludiamo la sezione con un esempio di applicazione dei problemi lineari all'analisi numerica (in particolare alla regressione lineare) e con un'osservazione sulla difficoltà computazionale della ricerca di soluzioni (possibili e ottimali) ad un dato problema lineare.

\begin{example}
  \label{es:minimi_quadrati}
  Dato un insieme di punti \(\{(x_i, y_i)\colon\ i=1,\ldots,n\}\) di \(\mathbb{R}^2\) è possibile ottenere l'equazione di una retta che si avvicina il più possibile ai punti dati usando il \textit{metodo dei minimi quadrati}.
  L'intera tecnica è basata sul cercare \(a,b\in\mathbb{R}\) tali che
  \begin{equation*}
    \sum_{i=1}^n(ax_i + b - y_i)^2
  \end{equation*}
  sia minimo. Un metodo alternativo (e per certi lati migliore) consiste nel minimizzare direttamente la somma degli errori in valore assoluto:
  \begin{equation*}
    \tag{\(*\)}
    \sum_{i=1}^n|ax_i + b - y_i|.
  \end{equation*}
  Seppure questa quantità non sia lineare il problema può essere ridotto ad un problema lineare.
  Si consideri infatti il problema
  \begin{equation*}
    \begin{array}{lll}
      \text{Minimizza} & e_1 + \ldots + e_n &\\
      \text{rispetto ai vincoli} & e_i\geq ax_i + b - y_i &\\
                       & e_i\geq -(ax_i + b - y_i) &,\ \text{con}\ i = 1,\ldots, n.\\
    \end{array}
  \end{equation*}
  Le variabili qui sono \(a, b, e_1,\ldots, e_n\).
  Ogni \(e_i\) è una variabile ausiliaria che rappresenta l'errore relativo al punto \((x_i, y_i)\); infatti i vincoli ci assicurano che
  \[e_i\geq\max\big(ax_i + b - y_i, -(ax_i + b - y_i)\big) = |ax_i + b - y_i|.\]
  Nel caso di una soluzione ottimale tutte queste disuguaglianze devono valere come uguaglianze, altrimenti sarebbe possibile ridurre ulteriormente il corrispettivo \(e_i\).
  Ne consegue che una soluzione ottimale del problema fornisce una retta che minimizza \((*)\).
\end{example}

\begin{remark}
  \label{oss:binary_search}
  Consideriamo un problema lineare generico
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}.
    \end{array}
  \end{equation*}
  e supponiamo di sapere che \(0\leq \mathbf{c}^T\mathbf{x}\leq M\) per un \(M\in\mathbb{R}\) e ogni soluzione possibile \(\mathbf{x}\).
  Supponiamo inoltre di avere una procedura che ci permetta di sapere quando un arbitrario problema lineare è soddisfacibile.
  Allora possiamo approssimare il valore massimo di \(\mathbf{c}^T\mathbf{x}\) con una tecnica di ricerca binaria:
  \begin{enumerate}
  \item aggiungiamo al problema iniziale il vincolo \(\mathbf{c}^T\mathbf{x}\geq \frac{M}{2}\) e determiniamo se questo nuovo problema è soddisfacibile,
  \item se lo è allora il massimo di \(\mathbf{c}^T\mathbf{x}\) si trova tra \(\frac{M}{2}\) e \(M\), altrimenti si trova tra \(0\) e \(\frac{M}{2}\),
  \item ripetendo i passi (1) e (2) su questo nuovo intervallo possiamo molto velocemente localizzare il massimo.
  \end{enumerate}
  Questo ci dice che, computazionalmente, il problema di ricerca di una soluzione ottimale è tanto difficile quanto quello di ricerca di una soluzione qualunque.
\end{remark}

\section{Dualità e Lemma di Farkas}
\label{sec:dualità_e_lemma_di_farkas}
\epigraph{The Farkas Lemma is not exactly a difficult theorem, but it is not trivial either.}{\textit{Jiri Matousek, Bernd Garter}}

\subsection{Problemi lineari duali e Teorema di Dualità}
\label{subsec:duali}
\begin{remark}
  \label{oss:forma_quasi_equazionale}
  Consideriamo un problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P\)}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}.
    \end{array}
  \end{equation*}
  Per ogni \(x_i\) si considerino due variabili \(y_i, y'_i\) e si sostituisca ogni occorrenza di \(x_i\) nel problema con \(y_i - y'_i\).
  Aggiungendo i vincoli \(\mathbf{y},\mathbf{y'}\geq0\) si ottiene un secondo problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P^*\)}
      \text{Massimizza} & \mathbf{c'}^T\mathbf{z}\\
      \text{rispetto ai vincoli} & A'\mathbf{z}\leq\mathbf{b'}\\
                        & \mathbf{z}\geq0
    \end{array}
  \end{equation*}
  nelle variabili \(\mathbf{y}, \mathbf{y'}\) (qui rappresentate come un unico gruppo \(\mathbf{z}\) per semplicità) che è del tutto equivalente al sistema di partenza.
  Infatti una soluzione del primo fornisce una soluzione del secondo perché ogni numero reale può essere scritto come differenza di due reali positivi; mentre ogni soluzione del secondo fornisce una soluzione del primo ottenuta come \(x_i = y_i-y'_i\).
  È dunque sempre possibile supporre che un problema lineare sia dato nella forma \((P^*)\).
\end{remark}

\begin{remark}
  \label{oss:bound}
  Consideriamo ora un generico problema lineare con \(m\) vincoli e \(n\) variabili
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}\\
                        & \mathbf{x}\geq0.
    \end{array}
  \end{equation*}
  Combinando linearmente gli \(m\) vincoli con altrettanti coefficienti \emph{non negativi} \(y_1,\ldots,y_m\) si ottiene una nuova disuguaglianza
  \[d_1x_1+\ldots+d_nx_n\leq y_1b_1+\ldots+y_mb_m\]
  dove \(d_i = y_1a_{1,i}+\ldots+y_ma_{m,i}\). Se inoltre gli \(y_i\) vengono scelti in modo tale che \(c_j\leq d_j\) per ogni \(j=1,\ldots, n\) si ottiene il seguente limite superiore alla funzione obiettivo \(\mathbf{c}^T\mathbf{x}\)
  \[c_1x_1+\ldots+c_nx_n\leq d_1x_1+\ldots+d_nx_n\leq y_1b_1+\ldots+y_mb_m.\]
  Siamo ovviamente interessati a \emph{minimizzare} questo limite superiore il che ci porta alla prossima definizione.
\end{remark}

\begin{definition}
  \label{def:problema_duale}
  Dato un problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P\)}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}\\
                        & \mathbf{x}\geq0
    \end{array}
  \end{equation*}
  definiamo il suo \textbf{problema lineare duale}
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(\dual{P}\)}
      \text{Minimizza} & \mathbf{b}^T\mathbf{y}\\
      \text{rispetto ai vincoli} & A^T\mathbf{y}\geq\mathbf{c}\\
                        & \mathbf{y}\geq0.
    \end{array}
  \end{equation*}
\end{definition}

\begin{remark}
  \label{oss:duale_del_duale}
  Riscrivendo \((\dual{P})\) nella forma descritta nell'Osservazione \ref{oss:forma_quasi_equazionale} e computandone il duale si verifica che il problema ``biduale'' \((\dual{P})^{\text{op}}\) non è altro che il problema originale \((P)\).
  Possiamo dunque dire che \((P)\) e \((\dual{P})\) sono duali l'uno rispetto all'altro.
\end{remark}

\begin{proposition}[Teorema di Dualità debole]
  \label{prop:dualità_debole}
  Consideriamo ancora il problema lineare \((P)\).
  Per ogni soluzione possibile \(\mathbf{x}\) di \((P)\) e \(\mathbf{y}\) di \((\dual{P})\) si ha che
  \[\mathbf{c}^T\mathbf{x} \leq \mathbf{b}^T\mathbf{y}.\]
  Questo ci dice che
  \begin{enumerate}
  \item se \((P)\) è illimitato allora \((\dual{P})\) è insoddisfacibile,
  \item se \((\dual{P})\) è illimitato \footnote{Inferiormente, trattandosi di un problema di minimizzazione.} allora \((P)\) è insoddisfacibile.
  \end{enumerate}
\end{proposition}

\begin{proof}
  Ovvia dalla discussione fatta nell'Osservazione \ref{oss:bound} e dalla Definizione \ref{def:problema_duale} di problema duale.
\end{proof}

\begin{example}
  \label{es:problema_duale}
  Consideriamo il problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & 2x_1 + 3x_2\\
      \text{rispetto ai vincoli} & 4x_1 + 8x_2 \leq 12\\
                        & 2x_1 + x_2 \leq 3\\
                        & 3x_1 + 2x_2 \leq 4\\
                        & x_1, x_2\geq 0
    \end{array}
  \end{equation*}
  e il suo duale
  \begin{equation*}
    \begin{array}{ll}
      \text{Minimizza} & 12y_1 + 3y_2 + 4y_3\\
      \text{rispetto ai vincoli} & 4y_1 + 2y_2 + 3y_3 \geq 2\\
                       & 8y_1 + y_2 + 2 y_3 \geq 3\\
                       & y_1, y_2, y_3 \geq 0.
    \end{array}
  \end{equation*}
  Si può vedere che il problema duale è limitato ed ha soluzione ottimale \(4.75\).
  Anche il problema originale è limitato e, sorprendentemente, ha come soluzione ottimale anch'esso \(4.75\).
  Il Teorema di Dualità, che ora enuncieremo, mostra che questa situazione particolare è in realtà necessaria.
\end{example}

\begin{theorem}[Teorema di Dualità]
  \label{teo:dualità}
  Dato un problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P\)}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x} \leq \mathbf{b}\\
                        & \mathbf{x} \geq 0
    \end{array}
  \end{equation*}
  e il suo duale
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(\dual{P}\)}
      \text{Minimizza} & \mathbf{b}^T\mathbf{y}\\
      \text{rispetto ai vincoli} & A^T\mathbf{y} \geq \mathbf{c}\\
                        & \mathbf{y} \geq 0
    \end{array}
  \end{equation*}
  una e solo una delle seguenti affermazioni è vera:
  \begin{enumerate}
  \item \((P)\) e \((\dual{P})\) sono insoddisfacibili,
  \item \((P)\) è illimitato e \((\dual{P})\) è insoddisfacibile,
  \item \((\dual{P})\) è illimitato e \((P)\) è insoddisfacibile,
  \item \((P)\) e \((\dual{P})\) sono entrambi soddisfacibili e limitati.
    Inoltre se \(\mathbf{x}^*\) e \(\mathbf{y}^*\) sono due soluzioni ottimali (rispettivamente di \((P)\) e di \((\dual{P})\)) allora
    \[\mathbf{c}^T\mathbf{x}^* = \mathbf{b}^T\mathbf{y}^*.\]
  \end{enumerate}
\end{theorem}

Esistono diverse dimostrazioni di questo risultato.
Quella che presenteremo noi (si veda \S\ref{subsec:dim_dualità}) utilizza il Lemma di Farkas, che analizzeremo nella prossima sezione.

\subsection{Il Lemma di Farkas}
\label{subsec:lemma_di_farkas}

\begin{lemma}[Lemma di Farkas]
  \label{lemma:farkas}
  Data una matrice \(A\in\mathbb{R}^{m\times n}\) e un vettore \(\mathbf{b}\in\mathbb{R}^m\) allora una e solo una delle seguenti affermazioni è vera:
  \begin{enumerate}
  \item esiste un vettore \(\mathbf{x}\in\mathbb{R}^n\) tale che \(A\mathbf{x}=\mathbf{b}\) e \(\mathbf{x} \geq 0\),
  \item esiste un vettore \(\mathbf{y}\in\mathbb{R}^m\) tale che \(\mathbf{y}^TA \geq \mathbf{0}^T\) e \(\mathbf{y}^T\mathbf{x} \leq 0\).
  \end{enumerate}
\end{lemma}

Il Lemma di Farkas può essere interpretato geometricamente come un teorema di separazione per insiemi convessi.
Per farlo introduciamo la nozione di cono convesso generato da un insieme di vettori.

\begin{definition}
  \label{def:cono_convesso}
  Dati vettori \(\mathbf{a}_1,\ldots,\mathbf{a_n}\in\mathbb{R}^m\) il \textbf{cono convesso} da loro generato è l'insieme
  \[\big\{\lambda_1\mathbf{a}_1+\ldots+\lambda_n\mathbf{a}_n\colon\ \lambda_1,\ldots,\lambda_n \geq 0\big\}\]
  delle loro combinazioni lineari a coefficienti non negativi.
\end{definition}

Da questa definizione si ottiene facilmente che il Lemma di Farkas è equivalente alla seguente affermazione geometrica (si veda anche la Figura \ref{fig:farkas_geometrico}).

\begin{lemma}[Lemma di Farkas (versione geometrica)]
  \label{lemma:farkas_geometrico}
  Se \(\mathbf{a}_1,\ldots,\mathbf{a_n}\in\mathbb{R}^m\) sono vettori, \(C\) è il loro cono convesso e \(\mathbf{b}\in\mathbb{R}^m\) è un ulteriore vettore allora una e solo una delle seguenti affermazioni è vera:
  \begin{enumerate}
  \item \(\mathbf{b}\) appartiene a \(C\),
  \item esiste un iperpiano \(H=\{\mathbf{x}\in\mathbb{R}^M\colon\ \mathbf{y}^T\mathbf{x} = 0\}\) di \(\mathbb{R}^m\) che ``separa'' gli \(\mathbf{a}_i\) da \(\mathbf{b}\); cioè tale che \(\mathbf{y}^T\mathbf{a}_i \leq 0\) per ogni \(i=1,\ldots, n\) e \(\mathbf{y}^T\mathbf{b} > 0\).
  \end{enumerate}
\end{lemma}

\begin{figure}
  \begin{center}
    \begin{tikzpicture}[>=latex, line width=1]
      \fill[fill=blue!30!white, path fading=west, fading angle=-45]
        (0, 0) -- (-4, 1.33) -- (-1.25, 5) -- cycle;

      \draw[->] (0, 0) -- (-3, 1) node [above]{\(\textbf{a}_1\)};
      \draw[->] (0, 0) -- (-2, 2) node [above]{\(\textbf{a}_2\)};
      \draw[->] (0, 0) -- (-1, 4) node [above right]{\(\textbf{a}_3\)};
      \draw[->] (0, 0) -- (-1.5, 3.75) node [above]{\(\textbf{b}\)};
            
      \filldraw (0, 0) circle (2pt) node[above right]{\(\textbf{0}\)};
    \end{tikzpicture}\hspace{1cm}
    \begin{tikzpicture}[>=latex, line width=1]
      \fill[fill=blue!30!white, path fading=west, fading angle=-45]
      (0, 0) -- (-4, 1.33) -- (-1.25, 5) -- cycle;

      \draw[->] (0, 0) -- (-3, 1) node [above]{\(\textbf{a}_1\)};
      \draw[->] (0, 0) -- (-2, 2) node [above]{\(\textbf{a}_2\)};
      \draw[->] (0, 0) -- (-1, 4) node [above right]{\(\textbf{a}_3\)};
      \draw[->] (0, 0) -- (3, 2.25) node [above]{\(\textbf{b}\)};
      
      \filldraw (0, 0) circle (2pt) node[below right]{\(\textbf{0}\)};

      \draw[dashed] (-.5, -1) -- (2, 4) node[above right]{\(H\)};
    \end{tikzpicture}
  \end{center}
  \caption{}
  \label{fig:farkas_geometrico}
\end{figure}

La seguente proposizione verrà utilizzata nella dimostrazione del Teorema di Dualità.

\begin{proposition}
  \label{prop:farkas_2}
  Data una matrice \(A\in\mathbb{R}^{m\times n}\) e un vettore \(\textbf{b}\in \mathbb{R}^{m}\) il sistema \(A \textbf{x} \leq \textbf{b}\) ammette una soluzione \emph{non negativa} se e solo se ogni \(\mathbf{y}\in \mathbb{R}^m\) \emph{non negativo} che soddisfa \(\textbf{y}^TA \geq \textbf{0}^T\) soddisfa anche \(\textbf{y}^T \textbf{b} \geq 0\).
\end{proposition}

\begin{proof}
  Il Lemma di Farkas \ref{lemma:farkas} ci dice che un sistema \(A \textbf{x}= \textbf{b}\) ha una soluzione non negativa se e solo se per ogni \(\textbf{y}\) tale che \(\textbf{y}^TA \geq \textbf{0}^T\) si ha anche \(\textbf{y}^T \textbf{b} \geq 0\).
  Per poter applicare questo risultato costruiamo, data una matrice \(A\in \mathbb{R}^{m\times n}\), \(\overline{A}= (A|I_m)\) dove \(I_m\in \mathbb{R}^{m\times m}\) è la matrice identità.
  Si ha ora che \(A \textbf{x} \leq \textbf{b}\) ammette una soluzione non negativa se e solo se \(\overline{A} \mathbf{\overline{x}} = \textbf{b}\) ha una soluzione non negativa.

  Per quanto detto prima questo è equivalente a chiedere che ogni \(\textbf{y}\in \mathbb{R}^m\) che soddisfa \(\mathbf{y}^T\overline{A} \geq \textbf{0}^T\) soddisfi anche \(\mathbf{y}^T \textbf{b} \geq 0\).
  Ma richiedere che \(\textbf{y}^T\overline{A} \geq \textbf{0}^T\) è equivalente a richiedere \(\textbf{y}^TA \geq \textbf{0}^T\) e \(\textbf{y} \geq \textbf{0}\) (i.e. la non negatività di \(\textbf{y}\)); dunque abbiamo dimostrato l'equivalenza richiesta.
\end{proof}

Procederemo ora a dimostrare il Lemma di Farkas nella sua versione geometrica.
Occorre prima dimostrare il seguente ultieriore lemma.

\begin{lemma}
  \label{lemma:punto_vicino}
  Se \(C\) è un cono convesso generato dai vettori \(\mathbf{a}_1,\ldots,\mathbf{a}_n\) e \(\mathbf{b}\) è un punto non appartenente a \(C\) allora esiste un punto \(\mathbf{z}\in C\) tale che \(\|\mathbf{b} - \mathbf{z}\| \le \|\mathbf{b} - \mathbf{z}'\|\) per ogni \(\mathbf{z}'\in C\).
  Si veda anche la Figura \ref{fig:punto_vicino}.
\end{lemma}

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}[>=latex, line width=1]
      \fill[fill=blue!30!white, path fading=west, fading angle=-45]
        (0, 0) -- (-4, 0) -- (0, 4) -- cycle;

      \draw[->] (0, 0) -- (-3, 0) node [above]{\(\textbf{a}_1\)};
      \draw[->] (0, 0) -- (-1.5, 2) node [above right]{\(\textbf{a}_2\)};
      \draw[->] (0, 0) -- (0, 3) node [above right]{\(\textbf{a}_3\)};
      \draw[->] (0, 0) -- (2.5, 1) node [above]{\(\textbf{b}\)};

      \draw[dashed] (0, 1) -- (2.5, 1);

      \filldraw (0, 1) circle (2pt) node[above right]{\(\textbf{z}\)};
      \filldraw (0, 0) circle (2pt) node[below right]{\(\textbf{0}\)};
    \end{tikzpicture}\hspace{1cm}
  \end{center}
  \caption{}
  \label{fig:punto_vicino}
\end{figure}

Per dimostrare questo lemma sono necessari un altro paio di risultati.

\begin{proposition}
  \label{prop:closed_nearest_point}
  Sia \(X\subseteq \mathbb{R}^m\) un chiuso non vuoto e \(\mathbf{b}\) un punto arbitrario di \(\mathbb{R}^m\).
  Allora esiste un punto \(\mathbf{z}\in X\) tale che \(\|\mathbf{b}-\mathbf{z}\| \le \|\mathbf{b} - \mathbf{z}'\|\) per ogni \(\mathbf{z}'\in X\).
\end{proposition}

\begin{proof}
  Fissiamo un punto \(\mathbf{x}_0\in X\) e consideriamo l'insieme
  \[K=\big\{\mathbf{x}\in X\colon\ \|\mathbf{b} - \mathbf{x}\| \le \|\mathbf{b} - \mathbf{x}_0\|\big\}.\]
  Chiaramente se il risultato vale per \(K\) vale anche per \(X\).
  Inoltre \(K\) è l'intersezione di \(X\) (chiuso) con la palla chiusa di centro \(\mathbf{b}\) e raggio \(\|\mathbf{b} - \mathbf{x}_0\|\) dunque è un insieme chiuso e limitato e quindi compatto.
  Consideriamo ora la funzione
  \begin{equation*}
    \begin{array}{crcl}
      f\colon & K & \to & \mathbb{R}\\
              & \mathbf{x} & \mapsto & \|\mathbf{b} - \mathbf{x}\|
    \end{array}
  \end{equation*}
  che sappiamo essere continua.
  Siccome \(K\) è compatto \(f\) ha un minimo; esiste cioè un punto \(\mathbf{z}\in K\) tale che \(\|\mathbf{b} - \mathbf{z}\| \le \|\mathbf{b} - \mathbf{z}'\|\) per ogni \(\mathbf{z}'\in K\).
\end{proof}

\begin{definition}
  \label{def:cono_primitivo}
  Un \textbf{cono primitivo} di \(\mathbb{R}^m\) è un cono convesso generato da un insieme \emph{linearmente indipendente} di vettori.
\end{definition}

\begin{proposition}
  \label{prop:coni_primitivi_chiusi}
  Ogni cono primitivo \(P\) di \(\mathbb{R}^m\) è chiuso.
\end{proposition}

\begin{proof}
  Supponiamo che \(P\subseteq \mathbb{R}^m\) sia generato dai vettori (linearmente indipendenti) \(\mathbf{a}_1,\ldots,\mathbf{a}_k\) e sia \(P_0\) il cono chiuso di \(\mathbb{R}^k\) generato dai vettori \(\mathbf{e}_1,\ldots,\mathbf{e}_k\) della base canonica;  \(P_0\) è chiaramente chiuso.
  Consideriamo ora la mappa lineare
  \begin{equation*}
    \begin{array}{crcl}
      f\colon & \mathbb{R}^k & \to & \mathbb{R}^m\\
              & \mathbf{x} & \mapsto & x_1 \mathbf{a}_1 + \ldots + x_n \mathbf{a}_n
    \end{array}
  \end{equation*}
  che è iniettiva, per l'indipendenta degli \(\mathbf{a}_i\), e tale che \(P = f(P_0)\).
  Per iniettività \(f\) è un isomorfismo lineare tra \(\mathbb{R}^k\) e \(\im(f)\); possiamo dunque considerare la mappa lineare inversa \(f^{-1}\).
  Siccome le mappe lineari sono continue si ha che \(P\), in quanto controimmagine di \(P_0\) mediante \(f^{-1}\), è chiuso in \(\im(f)\).
  Ma \(\im(f)\) è un sottospazio lineare di \(\mathbb{R}^m\), dunque è chiuso in \(\mathbb{R}^m\) e così anche \(P\) lo è\footnote{Un sottoinsieme di un sottospazio topologico chiuso è chiuso se e solo se è chiuso nello spazio ambiente.}.
\end{proof}

\begin{proposition}
  \label{prop:unione_di_coni}
  Sia \(C\) un cono convesso di \(\mathbb{R}^m\) generato dai vettori \(\mathbf{a}_1,\ldots,\mathbf{a}_n\).
  Allora \(C\) si può esprimere come unione di un numero finito di coni primitivi.
\end{proposition}

\begin{proof}
  Per ogni \(\mathbf{x}\in C\) verifichiamo che esiste un cono principale generato da un sottoinsieme linearmente indipendente dei vettori \(\mathbf{a}_i\) che contiene \(\mathbf{x}\).

  Sia \(I\subseteq\{1,\ldots, n\}\) l'insieme più piccolo tale che \(\mathbf{x}\) appartenga al cono generato da \(A_I=\{\mathbf{a}_i\colon\ i\in I\}\).
  Abbiamo dunque che esistono coefficienti non negativi \(\alpha_i\) tali che \(\mathbf{x} = \sum_{i\in I}\alpha_i \mathbf{a}_i\).
  Ma possiamo addirittura richiedere che gli \(\alpha_i\) siano strettamente positivi; infatti se \(\alpha_j = 0\) si ha che \(\mathbf{x}\) è ancora contenuto nel cono generato da \(A_{I\setminus\{j\}}\).
  È ora sufficiente mostrare che \(A_I\) è un insieme linearmente indipendente di vettori.

  Per assurdo supponiamo che esistano coefficienti \(\beta_i\) tali che \(\sum_{i\in I}\beta_i \mathbf{a}_i=\mathbf{0}\) con almeno un \(\beta_j\neq0\).
  Allora esiste un \(t\in \mathbb{R}\) tale che tutte le espressioni \(\alpha_i - t\beta_i\) sono non negative e almeno una di queste è nulla (se esistono dei \(\beta_j\) strettamente positivi basta porre \(t\) uguale al minimo del valore \(\frac{\alpha_j}{\beta_j}\) computato su questi \(\beta_j\); se non esistono \(\beta_j\) strettamente positivi si può prima sostituire ogni \(\beta_i\) con \(-\beta_i\))).
  Si ottiene quindi  
  \begin{equation*}
    \mathbf{x} = \mathbf{x} - t\cdot\mathbf{0} = \sum_{i\in I}(\alpha_i - t\beta_i)\mathbf{a}_i
  \end{equation*}
  che esprime \(\mathbf{x}\) come combinazione lineare a coefficienti non negativi di meno di \(|I|\) vettori (perché una delle \(\alpha_i - t\beta_i\) è nulla) in contraddizione con la definizione di \(I\).
\end{proof}

\begin{proposition}
  \label{prop:coni_chiusi}
  Ogni cono convesso generato da un insieme finito di vettori \(\mathbf{a}_1,\ldots,\mathbf{a}_n\) è chiuso.
\end{proposition}

\begin{proof}
  Ovvia dalle Proposizioni \ref{prop:coni_primitivi_chiusi}, \ref{prop:unione_di_coni} e dal fatto che l'unione finita di chiusi è un chiuso.
\end{proof}

\begin{remark}
  \label{oss:numero_finito}
  La Proposizione \ref{prop:coni_chiusi} non si applica ai coni generati da un numero infinito di vettori.
  Si consideri infatti un disco chiuso \(D\) il cui bordo contenga l'origine \(\mathbf{0}\).
  Il cono generato da tutti i vettori che compongono \(D\) è un semipiano aperto unito al singoletto \(\{\mathbf{0}\}\) e questo non è un insieme chiuso.
\end{remark}

\begin{proof}[Dimostrazione del Lemma \ref{lemma:punto_vicino}]
  Immediata dalle Proposizioni \ref{prop:coni_chiusi} e \ref{prop:closed_nearest_point}.
\end{proof}

\begin{proof}[Dimostrazione del Lemma di Farkas (\ref{lemma:farkas_geometrico})]
  Se \(\mathbf{b}\in C\) non c'è nulla da dimostrare, dunque poniamoci nel caso \(\mathbf{b}\not\in C\).
  Sia quindi \(\mathbf{z}\) il punto di \(C\) più vicino a \(\mathbf{b}\) che esiste in virtù del Lemma \ref{lemma:punto_vicino}.
  Definiamo \(\mathbf{y} = \mathbf{z} - \mathbf{b}\) e controlliamo che l'iperpiano \(\big\{\mathbf{x}\in \mathbb{R}^m\colon\ \mathbf{y}^T \mathbf{x} = 0\big\}\) ``separa'' gli \(\mathbf{a}_i\) da \(\mathbf{b}\).

  Proviamo innanzitutto che \(\mathbf{y}^T \mathbf{z} = 0\) i.e. che \(\mathbf{y}\) e \(\mathbf{z}\) sono perpendicolari.
  Se \(\mathbf{z} = 0\) la cosa è ovvia; possiamo dunque assumere che \(\mathbf{z}\neq0\).
  Supponiamo per assurdo che \(\mathbf{z}\) non sia perpendicolare a \(\mathbf{y}\).
  Allora è possibile trovare un punto \(\mathbf{z}'\) sulla semiretta \(\big\{t \mathbf{z}\colon\ t \ge 0\big\}\) tale che \(\|\mathbf{b} - \mathbf{z}'\| < \|\mathbf{b} - \mathbf{z}\|\); ma \(\mathbf{z}\) è il punto di \(C\) più vicino a \(\mathbf{b}\), dunque abbiamo una contraddizione.

  Ora siccome \(\mathbf{b}\not\in C\) si ha \(\mathbf{y}\neq \mathbf{0}\) e dunque
  \begin{equation*}
    0 < \mathbf{y}^T \mathbf{y} = \mathbf{y}^T(\mathbf{z} - \mathbf{b}) = -\mathbf{y}^T \mathbf{b}.
  \end{equation*}
  Questo ci dice che \(\mathbf{y}^T \mathbf{b} < 0\).

  Infine consideriamo un generico \(\mathbf{x}\in C, \mathbf{x}\neq \mathbf{z}\) e dimostriamo che \(\mathbf{y}^T \mathbf{x} \ge 0\); una volta fatto questo, siccome gli \(\mathbf{a}_i\) sono tutti elementi di \(C\), la dimostrazione sarà completata.
  Si deve avere \((\mathbf{b} - \mathbf{z})^T(\mathbf{x} - \mathbf{z}) \le 0\) perché altrimenti l'angolo \(\sphericalangle \mathbf{b}\mathbf{z}\mathbf{x}\) sarebbe minore di \(90^\circ\) e allora ci sarebbe un punto \(\mathbf{z}'\) del segmento \(\mathbf{z}\mathbf{x}\) tale che \(\|\mathbf{b} - \mathbf{z}'\| < \|\mathbf{b} - \mathbf{z}\|\) contraddicendo ancora il fatto che \(\mathbf{z}\) sia il punto di \(C\) più vicino a \(\mathbf{b}\).
  Possiamo ora scrivere
  \begin{equation*}
    0 \ge (\mathbf{b} - \mathbf{z})^T(\mathbf{x} - \mathbf{z}) = -\mathbf{y}^T \mathbf{x} + \mathbf{y}^T \mathbf{z} = -\mathbf{y}^T \mathbf{x}
  \end{equation*}
  e dunque \(\mathbf{y}^T \mathbf{x} \ge 0\).
\end{proof}

\subsection{Dimostrazione del Teorema di Dualità dal Lemma di Farkas}
\label{subsec:dim_dualità}

Il seguente teorema fornisce condizioni sufficienti per la limitatezza di un problema lineare.

\begin{theorem}
  \label{teo:condizioni_sufficienti}
  Si consideri il seguente problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \text{Minimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x} \ge \mathbf{b}\\
                        & \mathbf{x} \ge 0.
    \end{array}
  \end{equation*}
  Se il problema ha una soluzione possibile e la funzione obiettivo \(\mathbf{c}^T\mathbf{x}\) è limitata \emph{inferiormente} sull'insieme di tutte le soluzioni possibili allora il problema ammette una soluzione ottimale.
\end{theorem}

\begin{proof}
  Una dimostrazione di questo teorema (in una forma leggermente diversa, ma equivalente) si può trovare in \cite[Theorem 4.2.3]{understanding_linear_programming}.
\end{proof}

\begin{proof}[Dimostrazione del Teorema di Dualità \ref{teo:dualità}]
  Come visto in \S\ref{sec:problemi_lineari} un problema lineare può essere insoddisfacibile, soddisfacibile limitato o soffisfacibile illimitato.
  Abbiamo dunque 3 possibilità per \((P)\) e altrettante per \((\dual{P})\) per un totale di 9 possibili combinazioni.
  La Proposizione \ref{prop:dualità_debole} ci permette subito di escludere i seguenti casi
  \begin{itemize}
  \item \((P)\) illimitato e \((\dual{P})\) limitato,
  \item \((\dual{P})\) illimitato e \((P)\) limitato.
  \item \((P)\) illimitato e \((\dual{P})\) illimitato.
  \end{itemize}
  Mostreremo ora che se \((P)\) è limitato allora anche \((\dual{P})\) lo è e dunque, per dualità, che se \((\dual{P})\) è limitato allora anche \((P)\) lo è.
  Questo esclude i casi
  \begin{itemize}
  \item \((P)\) limitato e \((\dual{P})\) insoddisfacibile,
  \item \((\dual{P})\) limitato e \((P)\) insoddisfacibile.
  \end{itemize}
  Una volta fatto questo i casi rimasti sono solo quelli elencati nell'enunciato del Lemma di Farkas \ref{lemma:farkas} e la dimostrazione è completa.

  Supponiamo quindi che \((P)\) sia limitato e siano: \(\textbf{x}^*\) una soluzione ottimale, \(\gamma = \textbf{c}^T \textbf{x}\) il valore di massimo e \(\varepsilon \geq 0\).
  Definiamo
  \[\overline{A} =
    \begin{pmatrix}
      A\\
      -\textbf{c}^T
    \end{pmatrix},\quad \overline{\textbf{b}_\varepsilon} =
    \begin{pmatrix}
      \mathbf{b}\\
      -\gamma-\varepsilon
    \end{pmatrix}\]
  e osserviamo che il sistema di disequazioni \(\overline{A}\textbf{x} \leq \overline{\textbf{b}_\varepsilon}\) è equivalente a
  \[A \textbf{x} \leq \textbf{b}, \textbf{c}^T \textbf{x} \geq \gamma + \varepsilon\]
  e dunque ammette una soluzione non negativa se e solo se \(\varepsilon = 0\).

  Siccome \(\overline{A}\textbf{x} \leq \overline{\textbf{b}_\varepsilon}\) non ha soluzioni non negative per \(\varepsilon>0\) la Proposizione \ref{prop:farkas_2} ci dice che esiste un vettore \(\overline{\textbf{y}}=(\textbf{u}, z)\in \mathbb{R}^{m+1}\) non negativo e tale che \(\overline{\textbf{y}}^TA \geq \textbf{0}^T\) ma \(\overline{\textbf{y}}^T\overline{\textbf{b}_\varepsilon} < 0\).
  Queste disuguaglianze possono essere riscritte, più chiaramente, come
  \begin{equation*}
    \tag{\(*\)}
    A^T \textbf{u} \geq z \textbf{c},\ \textbf{b}^T \textbf{u} < z(\gamma + \varepsilon).
  \end{equation*}
  Applicando la Proposizione \ref{prop:farkas_2} al caso \(\varepsilon=0\) si ottiene che il suddetto vettore \(\overline{\textbf{y}}\) deve soddisfare \(\overline{\textbf{y}}^T \overline{\textbf{b}_0} \geq 0\) e cioè che \(\textbf{b}^T \textbf{u} \ge z\gamma\).
  Da questa ultima relazione e da \((*)\) segue che bisogna avere \(z>0\) ed è quindi possibile definire il vettore \(\textbf{v} = \frac{1}{z}\textbf{u}\) che è chiaramente non negativo e tale che
  \begin{equation*}
    A^T \textbf{v} \geq \textbf{c},\ \textbf{b}^T \textbf{v} < \gamma + \varepsilon.
  \end{equation*}
  Abbiamo dunque ottenuto una soluzione di \((\dual{P})\) con valore della funzione obiettivo minore di \(\gamma + \varepsilon\).

  Per la Proposizione \ref{prop:dualità_debole} ogni soluzione di \((\dual{P})\) deve avere valore della funzione obiettivo maggiore o uguale a \(\gamma\).
  Si ha quindi che \((\dual{P})\) è un problema lineare soddisfacibile e con funzione obiettivo limitata inferiormente sull'insieme delle soluzioni possibili, dunque ammette una soluzione ottimale \(\textbf{y}^*\) (Teorema \ref{teo:condizioni_sufficienti}).
  Inoltre si ha che \(\gamma \le \textbf{b}^T\textbf{y}^* < \gamma + \varepsilon\) per ogni \(\varepsilon>0\) e quindi si deve avere \(\gamma = \textbf{c}^T \textbf{x}^* = \textbf{b}^T\textbf{y}^*\).
\end{proof}

\begin{thebibliography}{1}
\bibitem{understanding_linear_programming}
  Jiri Matousek \& Bernd Gartner,
  \textit{Understanding and Using Linear Programming},
  Springer,
  2007.
\end{thebibliography}
\end{document}
