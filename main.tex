\documentclass[italian, letter paper, 12pt, reqno]{article}
\usepackage[italian]{babel}

\setlength{\evensidemargin}{0.1in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\textwidth}{6.3in}
\setlength{\topmargin}{0.0in}
\setlength{\textheight}{8.5in}
\setlength{\headheight}{0in}

% Links and references.
\usepackage{xcolor}
\definecolor{Myblue}{rgb}{0,0,0.6}
\usepackage[a4paper,colorlinks,citecolor=Myblue,linkcolor=Myblue,urlcolor=Myblue,pdfpagemode=None]{hyperref}

% Necessities for math.
\usepackage{amsmath, amscd, amssymb, mathrsfs, accents, amsfonts, amsthm}

\newtheoremstyle{myteo}{\topsep}{\topsep}
	{}
	{}
	{\bfseries}
	{.}
	{2pt}
	{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}
\theoremstyle{myteo}

\newtheorem{theorem}{Teorema}[section]
\newtheorem{proposition}[theorem]{Proposizione}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollario}
\newtheorem{definition}[theorem]{Definizione}
\newtheorem{example}[theorem]{Esempio}
\newtheorem{remark}[theorem]{Osservazione}

\numberwithin{equation}{section}

\usepackage{tikz}
\usetikzlibrary{cd}

% Figures stuff.
\usepackage{caption}
\renewcommand{\thefigure}{\arabic{section}.\arabic{figure}}

% Lists stuff.
\usepackage{enumitem}
\setenumerate{label=(\arabic*)}

% Commands.
\newcommand{\dual}[1]{#1^{\text{op}}}

% Useless stuff.
\usepackage{epigraph}

\begin{document}
\title{Il Lemma di Farkas}
\author{Gabriele Rastello}
\maketitle
\tableofcontents

\section{Problemi lineari}
\label{sec:problemi_lineari}
\epigraph{Linear programming, surprisingly, is not directly related to computer programming.}{\textit{Jiri Matousek, Bernd Garter}}
Sono problemi lineari tutti quei problimi in cui ci si prefigge di trovare il valore massimo (o minimo) che una certa funzione lineare di \(n\) variabili può assumere, dato un qualche numero di vincoli (anche essi lineari) su queste variabili.
Prima di definire formalmente un problema lineare consideriamo un esempio.

\begin{example}
  \label{es:problema_lineare}
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & x_1 + x_2\\
      \text{rispetto ai vincoli} & x_1, x_2\geq 0\\
                        & x_2 - x_1 \leq 1\\
                        & x_1 + 6x_2 \leq 15\\
                        & 4x_1 - x_2 \leq 10
    \end{array}
  \end{equation*}
  In \(\mathbb{R}^2\) ogni vincolo individua un semipiano.
  La zona di \(\mathbb{R}^2\) su cui vogliamo massimizzare \(x_1+x_2\) è dunque l'intersezione di tutti questi semipiani ed è rappresentata in Figura \ref{fig:problema_lineare}.
  Osserviamo che quest'area non è vuota e che è un poligono convesso.
  Esiste dunque una coppia \((x_1^*,x_2^*)\) che massimizza \(x_1+x_2\); la coppia in questione può essere ottenuta cercando quale punto del poligono si trova ``più distante'' nella direzione di massima crescita della funzione (data dal suo gradiente \((1, 1)\)).
  Otteniamo così \(x_1^*=3, x_2^*=2\) e infine che il valore massimo di \(x_1+x_2\) rispetto ai vincoli dati è \(5\).

  \begin{figure}
    \begin{center}
      \begin{tikzpicture}
        \filldraw[fill=green!20!white]
        (0, 0) -- (0, 1) -- (9/7, 16/7) -- (3, 2) -- (10/4, 0);

        \draw[->] (-1, 0) -- (4, 0) node[right]{\(x_1\)};
        \draw[->] (0, -1) -- (0, 3.5) node[above]{\(x_2\)};

        \draw[domain = -.75:3.25] plot (\x, {\x + 1});
        \draw[domain = -.75:4] plot (\x, {(15 - \x)/6});
        \draw[domain = 2.25:3.5] plot (\x, {4*\x - 10});

        \filldraw (3, 2) circle (3pt) node[above right]{\((3,2)\)};
      \end{tikzpicture}
    \end{center}
    \caption{}
    \label{fig:problema_lineare}
  \end{figure}
\end{example}

\begin{definition}
  \label{def:problema_lineare}
  Un \textbf{problema lineare} consiste in una funzione lineare di \(n\) variabili detta \textbf{funzione obiettivo} (o \textbf{funzione di costo}) e in un insieme di \(m\) vincoli lineari.
  La funzione obiettivo ha la forma \(\mathbf{c}^T\mathbf{x} = c_1x_1+\ldots+c_nx_n\) per qualche \(\mathbf{c}\in\mathbb{R}^n\); lo stesso si applica ai vincoli.
  Dare un problema lineare è allora equivalente a dare un vettore \(\mathbf{c}\in\mathbb{R}^n\), una matrice \(A\in\mathbb{R}^{m\times n}\) e un vettore \(\mathbf{b}\in\mathbb{R}^m\).
  Scriveremo compattamente
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}.
    \end{array}
  \end{equation*}
\end{definition}

\begin{remark}
  \label{oss:definizione_generale}
  La Definizione \ref{def:problema_lineare} è del tutto generale.
  Infatti un problema di minimizzazione può essere trasformato in uno di massimizzazione cambiando segno alla funzione obiettivo.
  I vincoli espressi tramite un'uguaglianza \(\mathbf{a}^T\mathbf{x}=b\) sono equivalenti alla coppia di disuguaglianze \(\mathbf{a}^T\mathbf{x}\geq b\), \(\mathbf{a}^T\mathbf{x}\leq b\).
  Ed infine le disuguaglianze possono essere espresse tutte quante nella forma \(\mathbf{a}^T\mathbf{x}\leq b\).
\end{remark}

\begin{definition}
  \label{def:soluzioni}
  Un vettore \(\mathbf{x}\in\mathbb{R}^n\) che soddisfa tutti i vincoli di un problema lineare è una \textbf{soluzione possibile} per il problema.
  Un problema è \textbf{soddisfacibile} se ammette una soluzione possibile ed è \textbf{insoddisfacibile} altrimenti. Una soluzione possibile \(\mathbf{x}^*\in\mathbb{R}^n\) è una \textbf{soluzione ottimale} se \(\mathbf{c}^T\mathbf{x}^*\) è massimo tra i valori \(\mathbf{c}^T\mathbf{x}\) con \(\mathbf{x}\) soluzione possibile.
\end{definition}

\begin{remark}
  \label{oss:soluzioni_ottimali_illimitate}
  Va osservato che, generalmente, un sistema lineare può avere più di una soluzione ottimale; come esempio si considerino i vincoli dell'Esercizio \ref{es:problema_lineare} applicati però alla funzione obiettivo \(\frac{1}{6}x_1+x_2\).
  È inoltre vero che, anche se un sistema è soddisfacibile, possono non esistere soluzioni ottimali; come esempio basta rimuovere i vincoli \(x_1+6x_2\leq15\) e \(4x_1-x_2\leq10\) dall'Esercizio \ref{es:problema_lineare}.
\end{remark}

\begin{definition}
  \label{def:problemi_limitati_e_illimitati}
  Se un problema lineare ammette (almeno) una soluzione ottimale allora è detto \textbf{limitato}; se non ne ammette viene detto \textbf{illimitato}.
\end{definition}

Concludiamo la sezione con un esempio di applicazione dei problemi lineari all'analisi numerica (in particolare alla regressione lineare) e con un'osservazione sulla difficoltà computazionale della ricerca di soluzioni (possibili e ottimali) ad un dato problema lineare.

\begin{example}
  \label{es:minimi_quadrati}
  Dato un insieme di punti \(\{(x_i, y_i)\colon\ i=1,\ldots,n\}\) di \(\mathbb{R}^2\) è possibile ottenere l'equazione di una retta che si avvicina il più possibile ai punti dati usando il \textit{metodo dei minimi quadrati}.
  L'intera tecnica è basata sul cercare \(a,b\in\mathbb{R}\) tali che
  \begin{equation*}
    \sum_{i=1}^n(ax_i + b - y_i)^2
  \end{equation*}
  sia minimo. Un metodo alternativo (e per certi lati migliore) consiste nel minimizzare direttamente la somma degli errori in valore assoluto:
  \begin{equation*}
    \tag{\(*\)}
    \sum_{i=1}^n|ax_i + b - y_i|.
  \end{equation*}
  Seppure questa quantità non sia lineare il problema può essere ridotto ad un problema lineare.
  Si consideri infatti il problema
  \begin{equation*}
    \begin{array}{lll}
      \text{Minimizza} & e_1 + \ldots + e_n &\\
      \text{rispetto ai vincoli} & e_i\geq ax_i + b - y_i &\\
                       & e_i\geq -(ax_i + b - y_i) &,\ \text{con}\ i = 1,\ldots, n.\\
    \end{array}
  \end{equation*}
  Le variabili qui sono \(a, b, e_1,\ldots, e_n\).
  Ogni \(e_i\) è una variabile ausiliaria che rappresenta l'errore relativo al punto \((x_i, y_i)\); infatti i vincoli ci assicurano che
  \[e_i\geq\max\big(ax_i + b - y_i, -(ax_i + b - y_i)\big) = |ax_i + b - y_i|.\]
  Nel caso di una soluzione ottimale tutte queste disuguaglianze devono valere come uguaglianze, altrimenti sarebbe possibile ridurre ulteriormente il corrispettivo \(e_i\).
  Ne consegue che una soluzione ottimale del problema fornisce una retta che minimizza \((*)\).
\end{example}

\begin{remark}
  \label{oss:binary_search}
  Consideriamo un problema lineare generico
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}.
    \end{array}
  \end{equation*}
  e supponiamo di sapere che \(0\leq \mathbf{c}^T\mathbf{x}\leq M\) per un \(M\in\mathbb{R}\) e ogni soluzione possibile \(\mathbf{x}\).
  Supponiamo inoltre di avere una procedura che ci permetta di sapere quando un arbitrario problema lineare è soddisfacibile.
  Allora possiamo approssimare il valore massimo di \(\mathbf{c}^T\mathbf{x}\) con una tecnica di ricerca binaria:
  \begin{enumerate}
  \item aggiungiamo al problema iniziale il vincolo \(\mathbf{c}^T\mathbf{x}\geq \frac{M}{2}\) e determiniamo se questo nuovo problema è soddisfacibile,
  \item se lo è allora il massimo di \(\mathbf{c}^T\mathbf{x}\) si trova tra \(\frac{M}{2}\) e \(M\), altrimenti si trova tra \(0\) e \(\frac{M}{2}\),
  \item ripetendo i passi (1) e (2) su questo nuovo intervallo possiamo molto velocemente localizzare il massimo.
  \end{enumerate}
  Questo ci dice che, computazionalmente, il problema di ricerca di una soluzione ottimale è tanto difficile quanto quello di ricerca di una soluzione qualunque.
\end{remark}

\section{Dualità e Lemma di Farkas}
\label{sec:dualità_e_lemma_di_farkas}

\subsection{Problemi lineari duali e Teorema di Dualità}
\label{subsec:duali}
\begin{remark}
  \label{oss:forma_quasi_equazionale}
  Consideriamo un problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P\)}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}.
    \end{array}
  \end{equation*}
  Per ogni \(x_i\) si considerino due variabili \(y_i, y'_i\) e si sostituisca ogni occorrenza di \(x_i\) nel problema con \(y_i - y'_i\).
  Aggiungendo i vincoli \(\mathbf{y},\mathbf{y'}\geq0\) si ottiene un secondo problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P^*\)}
      \text{Massimizza} & \mathbf{c'}^T\mathbf{z}\\
      \text{rispetto ai vincoli} & A'\mathbf{z}\leq\mathbf{b'}\\
                        & \mathbf{z}\geq0
    \end{array}
  \end{equation*}
  nelle variabili \(\mathbf{y}, \mathbf{y'}\) (qui rappresentate come un unico gruppo \(\mathbf{z}\) per semplicità) che è del tutto equivalente al sistema di partenza.
  Infatti una soluzione del primo fornisce una soluzione del secondo perché ogni numero reale può essere scritto come differenza di due reali positivi; mentre ogni soluzione del secondo fornisce una soluzione del primo ottenuta come \(x_i = y_i-y'_i\).
  È dunque sempre possibile supporre che un problema lineare sia dato nella forma \((P^*)\).
\end{remark}

\begin{remark}
  \label{oss:bound}
  Consideriamo ora un generico problema lineare con \(m\) vincoli e \(n\) variabili
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}\\
                        & \mathbf{x}\geq0.
    \end{array}
  \end{equation*}
  Combinando linearmente gli \(m\) vincoli con altrettanti coefficienti \emph{non negativi} \(y_1,\ldots,y_m\) si ottiene una nuova disuguaglianza
  \[d_1x_1+\ldots+d_nx_n\leq y_1b_1+\ldots+y_mb_m\]
  dove \(d_i = y_1a_{1,i}+\ldots+y_ma_{m,i}\). Se inoltre gli \(y_i\) vengono scelti in modo tale che \(c_j\leq d_j\) per ogni \(j=1,\ldots, n\) si ottiene il seguente limite superiore alla funzione obiettivo \(\mathbf{c}^T\mathbf{x}\)
  \[c_1x_1+\ldots+c_nx_n\leq d_1x_1+\ldots+d_nx_n\leq y_1b_1+\ldots+y_mb_m.\]
  Siamo ovviamente interessati a \emph{minimizzare} questo limite superiore il che ci porta alla prossima definizione.
\end{remark}

\begin{definition}
  \label{def:problema_duale}
  Dato un problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P\)}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x}\leq\mathbf{b}\\
                        & \mathbf{x}\geq0
    \end{array}
  \end{equation*}
  definiamo il suo \textbf{problema lineare duale}
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(\dual{P}\)}
      \text{Minimizza} & \mathbf{b}^T\mathbf{y}\\
      \text{rispetto ai vincoli} & A^T\mathbf{y}\geq\mathbf{c}\\
                        & \mathbf{y}\geq0.
    \end{array}
  \end{equation*}
\end{definition}

\begin{remark}
  \label{oss:duale_del_duale}
  Riscrivendo \((\dual{P})\) nella forma descritta nell'Osservazione \ref{oss:forma_quasi_equazionale} e computandone il duale si verifica che il problema ``biduale'' \((\dual{P})^{\text{op}}\) non è altro che il problema originale \((P)\).
  Possiamo dunque dire che \((P)\) e \((\dual{P})\) sono duali l'uno rispetto all'altro.
\end{remark}

\begin{proposition}[Teorema di Dualità debole]
  \label{prop:dualità_debole}
  Consideriamo ancora il problema lineare \((P)\).
  Per ogni soluzione possibile \(\mathbf{x}\) di \((P)\) e \(\mathbf{y}\) di \((\dual{P})\) si ha che
  \[\mathbf{c}^T\mathbf{x} \leq \mathbf{b}^T\mathbf{y}.\]
  Questo ci dice che
  \begin{enumerate}
  \item se \((P)\) è illimitato allora \((\dual{P})\) è insoddisfacibile,
  \item se \((\dual{P})\) è illimitato \footnote{Inferiormente, trattandosi di un problema di minimizzazione.} allora \((P)\) è insoddisfacibile.
  \end{enumerate}
\end{proposition}

\begin{proof}
  Ovvia dalla discussione fatta nell'Osservazione \ref{oss:bound} e dalla Definizione \ref{def:problema_duale} di problema duale.
\end{proof}

\begin{example}
  \label{es:problema_duale}
  Consideriamo il problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \text{Massimizza} & 2x_1 + 3x_2\\
      \text{rispetto ai vincoli} & 4x_1 + 8x_2 \leq 12\\
                        & 2x_1 + x_2 \leq 3\\
                        & 3x_1 + 2x_2 \leq 4\\
                        & x_1, x_2\geq 0
    \end{array}
  \end{equation*}
  e il suo duale
  \begin{equation*}
    \begin{array}{ll}
      \text{Minimizza} & 12y_1 + 3y_2 + 4y_3\\
      \text{rispetto ai vincoli} & 4y_1 + 2y_2 + 3y_3 \geq 2\\
                       & 8y_1 + y_2 + 2 y_3 \geq 3\\
                       & y_1, y_2, y_3 \geq 0.
    \end{array}
  \end{equation*}
  Si può vedere che il problema duale è limitato ed ha soluzione ottimale \(4.75\).
  Anche il problema originale è limitato e, sorprendentemente, ha come soluzione ottimale anch'esso \(4.75\).
  Il Teorema di Dualità, che ora enuncieremo, mostra che questa situazione particolare è in realtà necessaria.
\end{example}

\begin{theorem}[Teorema di Dualità]
  \label{teo:dualità}
  Dato un problema lineare
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(P\)}
      \text{Massimizza} & \mathbf{c}^T\mathbf{x}\\
      \text{rispetto ai vincoli} & A\mathbf{x} \leq \mathbf{b}\\
                        & \mathbf{x} \geq 0
    \end{array}
  \end{equation*}
  e il suo duale
  \begin{equation*}
    \begin{array}{ll}
      \tag{\(\dual{P}\)}
      \text{Minimizza} & \mathbf{b}^T\mathbf{y}\\
      \text{rispetto ai vincoli} & A^T\mathbf{y} \geq \mathbf{c}\\
                        & \mathbf{y} \geq 0
    \end{array}
  \end{equation*}
  una e solo una delle seguenti affermazioni è vera:
  \begin{enumerate}
  \item \((P)\) e \((\dual{P})\) sono insoddisfacibili,
  \item \((P)\) è illimitato e \((\dual{P})\) è insoddisfacibile,
  \item \((\dual{P})\) è illimitato e \((P)\) è insoddisfacibile,
  \item \((P)\) e \((\dual{P})\) sono entrambi soddisfacibili e limitati.
    Inoltre se \(\mathbf{x}^*\) e \(\mathbf{y}^*\) sono due soluzioni ottimali (rispettivamente di \((P)\) e di \((\dual{P})\)) allora
    \[\mathbf{c}^T\mathbf{x} = \mathbf{b}^T\mathbf{y}.\]
  \end{enumerate}
\end{theorem}

Esistono diverse dimostrazioni di questo risultato.
Quella che presenteremo noi (si veda \ref{subsec:dim_dualità}) utilizza il Lemma di Farkas, che analizzeremo nella prossima sezione.

\subsection{Il Lemma di Farkas}
\label{subsec:lemma_di_farkas}

\subsection{Dimostrazione del Teorema di Dualità dal Lemma di Farkas}
\label{subsec:dim_dualità}

\begin{thebibliography}{1}
\bibitem{understanding_linear_programming}
  Jiri Matousek \& Bernd Gartner,
  \textit{Understanding and Using Linear Programming},
  Springer,
  2007.
\end{thebibliography}
\end{document}
